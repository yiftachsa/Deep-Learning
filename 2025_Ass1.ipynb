{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiftachsa/Deep-Learning/blob/main/2025_Ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzYrNKEAmkQE"
      },
      "source": [
        "Deep Learning - Assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6zyr06YohDE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C00wKXkbmoLt"
      },
      "source": [
        "Question 1 - Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k4n4SsHmfNd"
      },
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "  \"\"\"\n",
        "  layer_dims - python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "  Returns:\n",
        "  parameters -python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                  Wl - weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                  bl - bias vector of shape (layer_dims[l], 1)\n",
        "  \"\"\"\n",
        "  parameters = {}\n",
        "  L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "  for l in range(1, L):\n",
        "      parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*np.sqrt(2/layer_dims[l-1])\n",
        "      parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "      assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "      assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "\n",
        "  return parameters\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "  W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "  b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "  Returns:\n",
        "  Z -- the input of the activation function, also called pre-activation parameter\n",
        "  cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "  \"\"\"\n",
        "\n",
        "  Z = np.dot(W,A)+b\n",
        "\n",
        "  cache = (A, W, b)\n",
        "\n",
        "  return Z, cache\n",
        "\n",
        "def softmax(Z):\n",
        "  # A = np.exp(Z)/(np.sum(np.exp(Z)))\n",
        "  exp = np.exp(Z -np.max(Z))\n",
        "  sum_exp = exp.sum(axis=0, keepdims=True)\n",
        "\n",
        "  A = exp/sum_exp\n",
        "\n",
        "  activation_cache  = (Z)\n",
        "\n",
        "  return A, activation_cache\n",
        "\n",
        "def relu(Z):\n",
        "\n",
        "  A = np.maximum(0,Z)\n",
        "\n",
        "  activation_cache  = (Z)\n",
        "\n",
        "  return A, activation_cache\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "  W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "  b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "  activation -- the activation to be used in this layer, stored as a text string: \"softmax\" or \"relu\"\n",
        "\n",
        "  Returns:\n",
        "  A -- the output of the activation function, also called the post-activation value\n",
        "  cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "            stored for computing the backward pass efficiently\n",
        "  \"\"\"\n",
        "  Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "  if activation == \"softmax\":\n",
        "      A, activation_cache = softmax(Z)\n",
        "  elif activation == \"relu\":\n",
        "      A, activation_cache = relu(Z)\n",
        "\n",
        "  cache = (linear_cache, activation_cache)\n",
        "\n",
        "  return A, cache\n",
        "\n",
        "def L_model_forward(X, parameters, use_batchnorm):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  X -- data, numpy array of shape (input size, number of examples)\n",
        "  parameters -- output of initialize_parameters_deep()\n",
        "\n",
        "  Returns:\n",
        "  AL -- last post-activation value\n",
        "  caches -- list of caches containing:\n",
        "              every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "  \"\"\"\n",
        "  caches = []\n",
        "  A = X\n",
        "  L = len(parameters) // 2                  # number of layers in the neural network\n",
        "\n",
        "  # [LINEAR -> RELU]*(L-1).\n",
        "  for l in range(1, L):\n",
        "      A_prev = A\n",
        "\n",
        "      A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "      if use_batchnorm:\n",
        "        A = apply_batchnorm(A)\n",
        "      caches.append(cache)\n",
        "\n",
        "\n",
        "  # LINEAR -> SOFTMAX\n",
        "  AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"softmax\")\n",
        "  caches.append(cache)\n",
        "\n",
        "  return AL, caches\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "  Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "  Returns:\n",
        "  cost -- cross-entropy cost\n",
        "  \"\"\"\n",
        "\n",
        "  m = Y.shape[1]\n",
        "\n",
        "  # Compute loss\n",
        "\n",
        "  logprobs = np.multiply(np.log(AL),Y)\n",
        "  sigma_c = np.sum(logprobs,axis=1)\n",
        "\n",
        "  cost = - (1 / m) * np.sum(sigma_c)\n",
        "  # cost = -np.mean(Y * np.log(AL + 1e-8))\n",
        "\n",
        "  return cost\n",
        "\n",
        "def apply_batchnorm(A):\n",
        "  epsilon = 0.00001\n",
        "  m = A.shape[1]\n",
        "  myu = 1/m * np.sum(A,axis=1)\n",
        "  sigma_sqr = 1/m * np.sum(np.square(A-myu), axis=1)\n",
        "\n",
        "  NA = (A-myu) / np.sqrt(sigma_sqr + epsilon)\n",
        "  return NA\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqOUpVhc8xjX"
      },
      "source": [
        "Question 2 - Backward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q_Idame89BV"
      },
      "source": [
        "def Linear_backward(dZ, cache):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "  cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "  Returns:\n",
        "  dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "  dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "  db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "  \"\"\"\n",
        "  A_prev, W, b = cache\n",
        "  m = A_prev.shape[1]\n",
        "\n",
        "  dW = (1/m) *np.dot(dZ, A_prev.T)\n",
        "  db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
        "  dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "  return dA_prev, dW, db\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  dA -- post-activation gradient for current layer l\n",
        "  cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "  activation -- the activation to be used in this layer, stored as a text string: \"softmax\" or \"relu\"\n",
        "\n",
        "  Returns:\n",
        "  dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "  dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "  db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "  \"\"\"\n",
        "  linear_cache, activation_cache = cache\n",
        "\n",
        "  if activation == \"relu\":\n",
        "      dZ = relu_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = Linear_backward(dZ, linear_cache)\n",
        "\n",
        "  elif activation == \"softmax\":\n",
        "      dZ = softmax_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = Linear_backward(dZ, linear_cache)\n",
        "\n",
        "  return dA_prev, dW, db\n",
        "\n",
        "def relu_backward(dA, activation_cache):\n",
        "  Z = activation_cache\n",
        "  g_tag_Z = np.where(Z > 0, 1, 0)\n",
        "  dZ = dA*g_tag_Z\n",
        "  return dZ\n",
        "#relu_backward([],np.array([[2,0,1],[1,-2,2]]))\n",
        "def softmax_backward(dA, activation_cache):\n",
        "  Z,Y = activation_cache\n",
        "  A,_ = softmax(Z)\n",
        "  dZ = A-Y\n",
        "  return dZ\n",
        "\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "  Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "  caches -- list of caches containing:\n",
        "              every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "              the cache of linear_activation_forward() with \"softmax\" (it's caches[L-1])\n",
        "\n",
        "  Returns:\n",
        "  grads -- A dictionary with the gradients\n",
        "            grads[\"dA\" + str(l)] = ...\n",
        "            grads[\"dW\" + str(l)] = ...\n",
        "            grads[\"db\" + str(l)] = ...\n",
        "  \"\"\"\n",
        "  grads = {}\n",
        "  L = len(caches) # the number of layers\n",
        "  m = AL.shape[1]\n",
        "\n",
        "  # Initializing the backpropagation\n",
        "  # dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "  dAL =[]\n",
        "\n",
        "  # Lth layer (SOFTMAX -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "  current_cache = caches[L-1]\n",
        "  linear_cache, activation_cache = current_cache\n",
        "  activation_cache = (activation_cache, Y)\n",
        "  current_cache = (linear_cache, activation_cache)\n",
        "  grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"softmax\")\n",
        "\n",
        "\n",
        "  # Loop from l=L-2 to l=0\n",
        "  for l in reversed(range(L-1)):\n",
        "      # lth layer: (RELU -> LINEAR) gradients.\n",
        "      # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
        "\n",
        "      current_cache = caches[l]\n",
        "\n",
        "      dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation=\"relu\")\n",
        "      grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "      grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "      grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "  return grads\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  parameters -- python dictionary containing your parameters\n",
        "  grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "\n",
        "  Returns:\n",
        "  parameters -- python dictionary containing your updated parameters\n",
        "                parameters[\"W\" + str(l)] = ...\n",
        "                parameters[\"b\" + str(l)] = ...\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "  # Update rule for each parameter. Use a for loop.\n",
        "  for l in range(L):\n",
        "      parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "      parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ZqFK7ovbBT"
      },
      "source": [
        "Question 3 - L-layer model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgeOJO5ODcbR"
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size, X_val, Y_val,use_batchnorm=False):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "  Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "  layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "  learning_rate -- learning rate of the gradient descent update rule\n",
        "  num_iterations -- number of iterations of the optimization loop\n",
        "  print_cost -- if True, it prints the cost every 100 steps\n",
        "\n",
        "  Returns:\n",
        "  parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(1)\n",
        "  costs = []\n",
        "\n",
        "  m = X.shape[1]\n",
        "\n",
        "  # Parameters initialization. (â‰ˆ 1 line of code)\n",
        "  parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "\n",
        "  # gradient descent\n",
        "  for i in range(0, num_iterations):\n",
        "      for batch_start_idx in range(0,m,batch_size):\n",
        "        batch_end_idx = batch_start_idx+batch_size\n",
        "\n",
        "        batch_X = X[:,batch_start_idx:batch_end_idx]\n",
        "        batch_Y = Y[:,batch_start_idx:batch_end_idx]\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SOFTMAX\n",
        "        AL, caches = L_model_forward(batch_X, parameters, use_batchnorm)\n",
        "\n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, batch_Y)\n",
        "\n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, batch_Y, caches)\n",
        "\n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "\n",
        "      ### CAN BE DONE AT AN END OF AN EPOCH\n",
        "      # Print the cost every 100 training example\n",
        "      # batch_idx = batch_start_idx/batch_size\n",
        "      # TODO: change to 100!!\n",
        "      if i % 1 == 0:\n",
        "          print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "      if i % 1 == 0:\n",
        "          costs.append(cost)\n",
        "      # predict on validation set. stop if the change is insignificant (delta < 0.0001)\n",
        "      # val_accuracy = Predict(X_val, Y_val, parameters)\n",
        "      # if val_accuracy < 0.0001:\n",
        "      #   break\n",
        "\n",
        "  # plot the cost\n",
        "  plt.plot(np.squeeze(costs))\n",
        "  plt.ylabel('cost')\n",
        "  plt.xlabel('iterations (per hundreds)')\n",
        "  plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "  plt.show()\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMTrcOr7xwII"
      },
      "source": [
        "Question 3 - predict\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfmeqC0QVrOK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bTvyM2fxyyI"
      },
      "source": [
        "def categorical_accuracy(Y,Y_hat):\n",
        "  print(\"Y.shape\", Y.shape)\n",
        "  print(\"Y_hat.shape\", Y_hat.shape)\n",
        "  m = CategoricalAccuracy()\n",
        "  m.update_state(Y, Y_hat)\n",
        "  return m.result().numpy()\n",
        "\n",
        "def Predict(X,Y, parameters):\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  Y_hat, _ = L_model_forward(X, parameters, False)\n",
        "  return categorical_accuracy(Y, Y_hat)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFOSlEDw17x7"
      },
      "source": [
        "Question 4 - MNIST\n",
        "\n",
        "a."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvB3m5HB1-Mg",
        "outputId": "9e0fcc93-8377-4f92-fb1a-bf8d92db4503"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], 784)\n",
        "X_test = X_test.reshape(X_test.shape[0], 784)\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# fig=plt.figure(figsize=(20.,8.))\n",
        "# for i in range(10):\n",
        "#   for sample_idx in range(len(Y_train)):\n",
        "#     if Y_train[sample_idx] == i:\n",
        "#       fig.add_subplot(2,5,i+1)\n",
        "#       plt.title(\"class: {}\".format(i))\n",
        "#       plt.imshow(X_train[sample_idx].reshape(28,28), cmap='gray')\n",
        "#       break\n",
        "# plt.show()\n",
        "\n",
        "# One hot encoding the Y labels\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(range(0,10))\n",
        "Y_train = lb.transform(Y_train)\n",
        "Y_test = lb.transform(Y_test)\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1)\n",
        "\n",
        "# print(f\"X_train, Y_train: {X_train.shape, Y_train.shape}\")\n",
        "# print(f\"X_val, Y_val: {X_val.shape, Y_val.shape}\")\n",
        "# print(f\"X_test, Y_test: {X_test.shape, Y_test.shape}\")\n",
        "\n",
        "# Reshape the training and test examples\n",
        "X_train = X_train.T\n",
        "X_val = X_val.T\n",
        "X_test = X_test.T\n",
        "Y_train = Y_train.T\n",
        "Y_val = Y_val.T\n",
        "Y_test = Y_test.T\n",
        "\n",
        "print(f\"X_train, Y_train: {X_train.shape, Y_train.shape}\")\n",
        "print(f\"X_val, Y_val: {X_val.shape, Y_val.shape}\")\n",
        "print(f\"X_test, Y_test: {X_test.shape, Y_test.shape}\")\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "X_train = X_train/255.\n",
        "X_val = X_val/255.\n",
        "X_test = X_test/255.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "X_train, Y_train: ((784, 48000), (10, 48000))\n",
            "X_val, Y_val: ((784, 12000), (10, 12000))\n",
            "X_test, Y_test: ((784, 10000), (10, 10000))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMDbGcX93-sO"
      },
      "source": [
        "b.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "8ZFduHLn4VWm",
        "outputId": "694066d4-eb40-41a8-9b33-38f9f639800b"
      },
      "source": [
        "layers_dims = (784, 20, 7, 5, 10)\n",
        "learning_rate = 0.009\n",
        "num_iterations = 10\n",
        "batch_size = 10\n",
        "params = L_layer_model(X_train, Y_train, layers_dims, learning_rate, num_iterations, batch_size, X_val, Y_val, False)\n",
        "accuracy = Predict(X_test, Y_test, params)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.447996\n",
            "Cost after iteration 1: 0.571337\n",
            "Cost after iteration 2: 0.575892\n",
            "Cost after iteration 3: 0.542754\n",
            "Cost after iteration 4: 0.735958\n",
            "Cost after iteration 5: 0.874618\n",
            "Cost after iteration 6: 0.875102\n",
            "Cost after iteration 7: 0.852971\n",
            "Cost after iteration 8: 0.773602\n",
            "Cost after iteration 9: 0.738035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Y.shape (10, 10000)\n",
            "Y_hat.shape (10, 10000)\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}